{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc73932d-b70a-4862-b9fd-b493266caac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4cf78db-c5b9-407f-b6a0-ae2dc8aa28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar características del conjunto de entrenamiento\n",
    "X_train = np.loadtxt('caracteristicas_train.csv', delimiter=',')\n",
    "\n",
    "# Cargar etiquetas del conjunto de entrenamiento\n",
    "y_train = np.loadtxt('labels_train.txt', dtype=int)\n",
    "\n",
    "# Cargar características del conjunto de prueba\n",
    "X_test = np.loadtxt('caracteristicas_test.csv', delimiter=',')\n",
    "\n",
    "# Cargar etiquetas del conjunto de prueba\n",
    "y_test = np.loadtxt('labels_test.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fec05016-aacc-48d8-8303-7f27bcb2e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6b28600-678c-4be7-b9ea-9453936cee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el clasificador MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10000,), activation='relu', solver='adam', max_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc60445c-db14-4bed-a3e7-5ae273751610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento en lotes sucesivos y monitoreo de estadísticas\n",
    "num_batches = 100  # Número de lotes sucesivos\n",
    "batch_size = len(X_train) // num_batches  # Tamaño del lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0583a2d8-6ed7-46e4-88e8-ee15059978bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lote: 1\n",
      "Pérdida en entrenamiento: 1.7997363506508908\n",
      "Precisión en entrenamiento: 0.32875\n",
      "------------------------\n",
      "Lote: 2\n",
      "Pérdida en entrenamiento: 1.720792848534569\n",
      "Precisión en entrenamiento: 0.33625\n",
      "------------------------\n",
      "Lote: 3\n",
      "Pérdida en entrenamiento: 1.682545369530089\n",
      "Precisión en entrenamiento: 0.32\n",
      "------------------------\n",
      "Lote: 4\n",
      "Pérdida en entrenamiento: 1.872761207209671\n",
      "Precisión en entrenamiento: 0.33125\n",
      "------------------------\n",
      "Lote: 5\n",
      "Pérdida en entrenamiento: 1.5871704791254073\n",
      "Precisión en entrenamiento: 0.34875\n",
      "------------------------\n",
      "Lote: 6\n",
      "Pérdida en entrenamiento: 1.452695161849921\n",
      "Precisión en entrenamiento: 0.33125\n",
      "------------------------\n",
      "Lote: 7\n",
      "Pérdida en entrenamiento: 1.4921287748205883\n",
      "Precisión en entrenamiento: 0.32625\n",
      "------------------------\n",
      "Lote: 8\n",
      "Pérdida en entrenamiento: 1.280215701630392\n",
      "Precisión en entrenamiento: 0.3325\n",
      "------------------------\n",
      "Lote: 9\n",
      "Pérdida en entrenamiento: 1.56593214464615\n",
      "Precisión en entrenamiento: 0.34\n",
      "------------------------\n",
      "Lote: 10\n",
      "Pérdida en entrenamiento: 1.8614288730017885\n",
      "Precisión en entrenamiento: 0.35375\n",
      "------------------------\n",
      "Lote: 11\n",
      "Pérdida en entrenamiento: 1.2015130820394972\n",
      "Precisión en entrenamiento: 0.35375\n",
      "------------------------\n",
      "Lote: 12\n",
      "Pérdida en entrenamiento: 1.9015065090687917\n",
      "Precisión en entrenamiento: 0.3525\n",
      "------------------------\n",
      "Lote: 13\n",
      "Pérdida en entrenamiento: 1.6041665193922046\n",
      "Precisión en entrenamiento: 0.3725\n",
      "------------------------\n",
      "Lote: 14\n",
      "Pérdida en entrenamiento: 1.6567389788316442\n",
      "Precisión en entrenamiento: 0.39\n",
      "------------------------\n",
      "Lote: 15\n",
      "Pérdida en entrenamiento: 1.7167491231308705\n",
      "Precisión en entrenamiento: 0.4175\n",
      "------------------------\n",
      "Lote: 16\n",
      "Pérdida en entrenamiento: 1.9761609922764864\n",
      "Precisión en entrenamiento: 0.4375\n",
      "------------------------\n",
      "Lote: 17\n",
      "Pérdida en entrenamiento: 1.5623464203021182\n",
      "Precisión en entrenamiento: 0.4475\n",
      "------------------------\n",
      "Lote: 18\n",
      "Pérdida en entrenamiento: 1.2730374380629883\n",
      "Precisión en entrenamiento: 0.45375\n",
      "------------------------\n",
      "Lote: 19\n",
      "Pérdida en entrenamiento: 1.3629369450894866\n",
      "Precisión en entrenamiento: 0.4525\n",
      "------------------------\n",
      "Lote: 20\n",
      "Pérdida en entrenamiento: 1.4179408794553694\n",
      "Precisión en entrenamiento: 0.44625\n",
      "------------------------\n",
      "Lote: 21\n",
      "Pérdida en entrenamiento: 1.6381973740249225\n",
      "Precisión en entrenamiento: 0.4475\n",
      "------------------------\n",
      "Lote: 22\n",
      "Pérdida en entrenamiento: 1.5110117022407348\n",
      "Precisión en entrenamiento: 0.44\n",
      "------------------------\n",
      "Lote: 23\n",
      "Pérdida en entrenamiento: 1.1996626090302414\n",
      "Precisión en entrenamiento: 0.4325\n",
      "------------------------\n",
      "Lote: 24\n",
      "Pérdida en entrenamiento: 1.1999605147370238\n",
      "Precisión en entrenamiento: 0.44875\n",
      "------------------------\n",
      "Lote: 25\n",
      "Pérdida en entrenamiento: 1.4855239557051616\n",
      "Precisión en entrenamiento: 0.45375\n",
      "------------------------\n",
      "Lote: 26\n",
      "Pérdida en entrenamiento: 1.6076223808305874\n",
      "Precisión en entrenamiento: 0.455\n",
      "------------------------\n",
      "Lote: 27\n",
      "Pérdida en entrenamiento: 2.0232206521382974\n",
      "Precisión en entrenamiento: 0.4575\n",
      "------------------------\n",
      "Lote: 28\n",
      "Pérdida en entrenamiento: 1.1203273957609088\n",
      "Precisión en entrenamiento: 0.4625\n",
      "------------------------\n",
      "Lote: 29\n",
      "Pérdida en entrenamiento: 1.50101276353767\n",
      "Precisión en entrenamiento: 0.46\n",
      "------------------------\n",
      "Lote: 30\n",
      "Pérdida en entrenamiento: 1.6052951911405609\n",
      "Precisión en entrenamiento: 0.46375\n",
      "------------------------\n",
      "Lote: 31\n",
      "Pérdida en entrenamiento: 2.0683788848739093\n",
      "Precisión en entrenamiento: 0.46\n",
      "------------------------\n",
      "Lote: 32\n",
      "Pérdida en entrenamiento: 1.1216413532405312\n",
      "Precisión en entrenamiento: 0.45875\n",
      "------------------------\n",
      "Lote: 33\n",
      "Pérdida en entrenamiento: 1.190407285981613\n",
      "Precisión en entrenamiento: 0.46125\n",
      "------------------------\n",
      "Lote: 34\n",
      "Pérdida en entrenamiento: 1.3969492804018262\n",
      "Precisión en entrenamiento: 0.45375\n",
      "------------------------\n",
      "Lote: 35\n",
      "Pérdida en entrenamiento: 1.664427127981221\n",
      "Precisión en entrenamiento: 0.4625\n",
      "------------------------\n",
      "Lote: 36\n",
      "Pérdida en entrenamiento: 1.715553911995305\n",
      "Precisión en entrenamiento: 0.46375\n",
      "------------------------\n",
      "Lote: 37\n",
      "Pérdida en entrenamiento: 1.9446629298501412\n",
      "Precisión en entrenamiento: 0.465\n",
      "------------------------\n",
      "Lote: 38\n",
      "Pérdida en entrenamiento: 1.3736932268359912\n",
      "Precisión en entrenamiento: 0.465\n",
      "------------------------\n",
      "Lote: 39\n",
      "Pérdida en entrenamiento: 1.30216090068392\n",
      "Precisión en entrenamiento: 0.46625\n",
      "------------------------\n",
      "Lote: 40\n",
      "Pérdida en entrenamiento: 1.9887369475061378\n",
      "Precisión en entrenamiento: 0.46375\n",
      "------------------------\n",
      "Lote: 41\n",
      "Pérdida en entrenamiento: 1.471478347383005\n",
      "Precisión en entrenamiento: 0.4625\n",
      "------------------------\n",
      "Lote: 42\n",
      "Pérdida en entrenamiento: 1.4342573250374016\n",
      "Precisión en entrenamiento: 0.4675\n",
      "------------------------\n",
      "Lote: 43\n",
      "Pérdida en entrenamiento: 1.0127664409045998\n",
      "Precisión en entrenamiento: 0.46\n",
      "------------------------\n",
      "Lote: 44\n",
      "Pérdida en entrenamiento: 1.3636240930053591\n",
      "Precisión en entrenamiento: 0.4475\n",
      "------------------------\n",
      "Lote: 45\n",
      "Pérdida en entrenamiento: 1.2951156070766354\n",
      "Precisión en entrenamiento: 0.4475\n",
      "------------------------\n",
      "Lote: 46\n",
      "Pérdida en entrenamiento: 1.4819659572526627\n",
      "Precisión en entrenamiento: 0.4475\n",
      "------------------------\n",
      "Lote: 47\n",
      "Pérdida en entrenamiento: 1.6703841275851183\n",
      "Precisión en entrenamiento: 0.44875\n",
      "------------------------\n",
      "Lote: 48\n",
      "Pérdida en entrenamiento: 1.3782577212993732\n",
      "Precisión en entrenamiento: 0.45375\n",
      "------------------------\n",
      "Lote: 49\n",
      "Pérdida en entrenamiento: 1.1698386834580095\n",
      "Precisión en entrenamiento: 0.455\n",
      "------------------------\n",
      "Lote: 50\n",
      "Pérdida en entrenamiento: 1.205768627811394\n",
      "Precisión en entrenamiento: 0.46\n",
      "------------------------\n",
      "Lote: 51\n",
      "Pérdida en entrenamiento: 1.3556273174212476\n",
      "Precisión en entrenamiento: 0.4525\n",
      "------------------------\n",
      "Lote: 52\n",
      "Pérdida en entrenamiento: 1.197157488049682\n",
      "Precisión en entrenamiento: 0.455\n",
      "------------------------\n",
      "Lote: 53\n",
      "Pérdida en entrenamiento: 1.183134436750555\n",
      "Precisión en entrenamiento: 0.45625\n",
      "------------------------\n",
      "Lote: 54\n",
      "Pérdida en entrenamiento: 1.2547775856065961\n",
      "Precisión en entrenamiento: 0.46375\n",
      "------------------------\n",
      "Lote: 55\n",
      "Pérdida en entrenamiento: 1.9455458685697764\n",
      "Precisión en entrenamiento: 0.465\n",
      "------------------------\n",
      "Lote: 56\n",
      "Pérdida en entrenamiento: 1.1855316675880667\n",
      "Precisión en entrenamiento: 0.4675\n",
      "------------------------\n",
      "Lote: 57\n",
      "Pérdida en entrenamiento: 1.2372689683046578\n",
      "Precisión en entrenamiento: 0.46125\n",
      "------------------------\n",
      "Lote: 58\n",
      "Pérdida en entrenamiento: 1.2164342266937838\n",
      "Precisión en entrenamiento: 0.48125\n",
      "------------------------\n",
      "Lote: 59\n",
      "Pérdida en entrenamiento: 1.6396620156235147\n",
      "Precisión en entrenamiento: 0.48\n",
      "------------------------\n",
      "Lote: 60\n",
      "Pérdida en entrenamiento: 1.7785639401885271\n",
      "Precisión en entrenamiento: 0.485\n",
      "------------------------\n",
      "Lote: 61\n",
      "Pérdida en entrenamiento: 1.2658231515529967\n",
      "Precisión en entrenamiento: 0.48125\n",
      "------------------------\n",
      "Lote: 62\n",
      "Pérdida en entrenamiento: 1.1617544408195395\n",
      "Precisión en entrenamiento: 0.48875\n",
      "------------------------\n",
      "Lote: 63\n",
      "Pérdida en entrenamiento: 1.2857435563340707\n",
      "Precisión en entrenamiento: 0.495\n",
      "------------------------\n",
      "Lote: 64\n",
      "Pérdida en entrenamiento: 1.3522714893418217\n",
      "Precisión en entrenamiento: 0.49375\n",
      "------------------------\n",
      "Lote: 65\n",
      "Pérdida en entrenamiento: 1.2867587602015802\n",
      "Precisión en entrenamiento: 0.4875\n",
      "------------------------\n",
      "Lote: 66\n",
      "Pérdida en entrenamiento: 1.9013560011828508\n",
      "Precisión en entrenamiento: 0.49\n",
      "------------------------\n",
      "Lote: 67\n",
      "Pérdida en entrenamiento: 1.1327870371152882\n",
      "Precisión en entrenamiento: 0.4925\n",
      "------------------------\n",
      "Lote: 68\n",
      "Pérdida en entrenamiento: 1.574270145860998\n",
      "Precisión en entrenamiento: 0.49\n",
      "------------------------\n",
      "Lote: 69\n",
      "Pérdida en entrenamiento: 1.1178176069216899\n",
      "Precisión en entrenamiento: 0.4975\n",
      "------------------------\n",
      "Lote: 70\n",
      "Pérdida en entrenamiento: 1.1446277397894626\n",
      "Precisión en entrenamiento: 0.4975\n",
      "------------------------\n",
      "Lote: 71\n",
      "Pérdida en entrenamiento: 1.4153626162757627\n",
      "Precisión en entrenamiento: 0.49375\n",
      "------------------------\n",
      "Lote: 72\n",
      "Pérdida en entrenamiento: 1.1440961327464614\n",
      "Precisión en entrenamiento: 0.4825\n",
      "------------------------\n",
      "Lote: 73\n",
      "Pérdida en entrenamiento: 1.2730582270429496\n",
      "Precisión en entrenamiento: 0.48875\n",
      "------------------------\n",
      "Lote: 74\n",
      "Pérdida en entrenamiento: 1.8207612336797023\n",
      "Precisión en entrenamiento: 0.49125\n",
      "------------------------\n",
      "Lote: 75\n",
      "Pérdida en entrenamiento: 1.4518553295304104\n",
      "Precisión en entrenamiento: 0.48625\n",
      "------------------------\n",
      "Lote: 76\n",
      "Pérdida en entrenamiento: 0.7361966921302578\n",
      "Precisión en entrenamiento: 0.47625\n",
      "------------------------\n",
      "Lote: 77\n",
      "Pérdida en entrenamiento: 1.1448357882114248\n",
      "Precisión en entrenamiento: 0.46625\n",
      "------------------------\n",
      "Lote: 78\n",
      "Pérdida en entrenamiento: 1.4851419537900181\n",
      "Precisión en entrenamiento: 0.47\n",
      "------------------------\n",
      "Lote: 79\n",
      "Pérdida en entrenamiento: 1.257314886982497\n",
      "Precisión en entrenamiento: 0.47875\n",
      "------------------------\n",
      "Lote: 80\n",
      "Pérdida en entrenamiento: 1.4166127339651255\n",
      "Precisión en entrenamiento: 0.4825\n",
      "------------------------\n",
      "Lote: 81\n",
      "Pérdida en entrenamiento: 0.8505615584418446\n",
      "Precisión en entrenamiento: 0.4925\n",
      "------------------------\n",
      "Lote: 82\n",
      "Pérdida en entrenamiento: 1.4618678717435423\n",
      "Precisión en entrenamiento: 0.49625\n",
      "------------------------\n",
      "Lote: 83\n",
      "Pérdida en entrenamiento: 1.100208628861017\n",
      "Precisión en entrenamiento: 0.495\n",
      "------------------------\n",
      "Lote: 84\n",
      "Pérdida en entrenamiento: 1.818125741758753\n",
      "Precisión en entrenamiento: 0.50625\n",
      "------------------------\n",
      "Lote: 85\n",
      "Pérdida en entrenamiento: 1.441091446987427\n",
      "Precisión en entrenamiento: 0.51\n",
      "------------------------\n",
      "Lote: 86\n",
      "Pérdida en entrenamiento: 1.5187628919663154\n",
      "Precisión en entrenamiento: 0.5025\n",
      "------------------------\n",
      "Lote: 87\n",
      "Pérdida en entrenamiento: 1.761035982706923\n",
      "Precisión en entrenamiento: 0.50375\n",
      "------------------------\n",
      "Lote: 88\n",
      "Pérdida en entrenamiento: 1.6969828922968717\n",
      "Precisión en entrenamiento: 0.5125\n",
      "------------------------\n",
      "Lote: 89\n",
      "Pérdida en entrenamiento: 1.2591112669294775\n",
      "Precisión en entrenamiento: 0.50625\n",
      "------------------------\n",
      "Lote: 90\n",
      "Pérdida en entrenamiento: 1.6381079372378853\n",
      "Precisión en entrenamiento: 0.51625\n",
      "------------------------\n",
      "Lote: 91\n",
      "Pérdida en entrenamiento: 1.2642494500646457\n",
      "Precisión en entrenamiento: 0.52375\n",
      "------------------------\n",
      "Lote: 92\n",
      "Pérdida en entrenamiento: 1.2275146163364017\n",
      "Precisión en entrenamiento: 0.52375\n",
      "------------------------\n",
      "Lote: 93\n",
      "Pérdida en entrenamiento: 1.2394742393346807\n",
      "Precisión en entrenamiento: 0.52625\n",
      "------------------------\n",
      "Lote: 94\n",
      "Pérdida en entrenamiento: 1.6175245646346297\n",
      "Precisión en entrenamiento: 0.525\n",
      "------------------------\n",
      "Lote: 95\n",
      "Pérdida en entrenamiento: 1.3786847129867017\n",
      "Precisión en entrenamiento: 0.5225\n",
      "------------------------\n",
      "Lote: 96\n",
      "Pérdida en entrenamiento: 1.5969990440832837\n",
      "Precisión en entrenamiento: 0.51125\n",
      "------------------------\n",
      "Lote: 97\n",
      "Pérdida en entrenamiento: 1.0425655641615172\n",
      "Precisión en entrenamiento: 0.50375\n",
      "------------------------\n",
      "Lote: 98\n",
      "Pérdida en entrenamiento: 1.2998019168163004\n",
      "Precisión en entrenamiento: 0.5025\n",
      "------------------------\n",
      "Lote: 99\n",
      "Pérdida en entrenamiento: 1.3076991841436822\n",
      "Precisión en entrenamiento: 0.50625\n",
      "------------------------\n",
      "Lote: 100\n",
      "Pérdida en entrenamiento: 1.1493738372883349\n",
      "Precisión en entrenamiento: 0.50625\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_batches):\n",
    "    start = i * batch_size\n",
    "    end = (i + 1) * batch_size\n",
    "    X_batch = X_train[start:end]\n",
    "    y_batch = y_train[start:end]\n",
    "    \n",
    "    # Entrenar en el lote actual\n",
    "    mlp.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
    "    \n",
    "    # Calcular estadísticas en el lote actual\n",
    "    train_loss = mlp.loss_\n",
    "    train_accuracy = mlp.score(X_train, y_train)\n",
    "    \n",
    "    print(\"Lote:\", i+1)\n",
    "    print(\"Pérdida en entrenamiento:\", train_loss)\n",
    "    print(\"Precisión en entrenamiento:\", train_accuracy)\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8137a4df-760e-4da5-b4f9-45b5edf58587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión en prueba: 0.38613861386138615\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_accuracy = mlp.score(X_test, y_test)\n",
    "print(\"Precisión en prueba:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810bebc6-1fd3-4120-bb9f-8c8311e27418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
